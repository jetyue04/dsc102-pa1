{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "285a4873-ff33-4ff1-b7d2-0032254484fe",
   "metadata": {},
   "source": [
    "## <font color='red'> INSTRUCTIONS </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89957ed8-c2d1-4592-8821-88806390d1cc",
   "metadata": {},
   "source": [
    "<b> \n",
    "1. Write your code only in cells below the \"WRITE CODE BELOW\" title. Do not modify the code below the \"DO NOT MODIFY\" title. <br>\n",
    "2. The expected data types of the output answers for each question are given in the last cell through assertion statements. Your answers must match these expected output data types. Hint: Many of the answers need to be a Python dictionary. Consider methods like to_dict() to convert a Pandas Series to a dictionary. <br>\n",
    "3. The answers are then written to a JSON file named my_results_PA1.json. You can compare this with the provided expected output file \"expected_results_PA1.json\". <br>\n",
    "4. After you complete writing your code, click \"Kernel -> Restart Kernel and Run All Cells\" on the top toolbar. There should NOT be any syntax/runtime errors, otherwise points will be deducted. <br>\n",
    "5. For submitting your solution, first download your notebook by clicking \"File -> Download\". Rename the file as &ltTEAM_ID&gt.ipynb\" and upload to Canvas.</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5f7e94-c5b1-494c-8aab-832242527a4e",
   "metadata": {},
   "source": [
    "## <font color='red'> DO NOT MODIFY </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76f3c8d7-690f-428b-982d-94265b4a7f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://172.31.43.150:8786' processes=6 threads=6, memory=23.42 GiB>\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from dask.distributed import Client\n",
    "import ctypes\n",
    "import numpy as np\n",
    "\n",
    "def trim_memory() -> int:\n",
    "    \"\"\"\n",
    "    helps to fix any memory leaks.\n",
    "    \"\"\"\n",
    "    libc = ctypes.CDLL(\"libc.so.6\")\n",
    "    return libc.malloc_trim(0)\n",
    "\n",
    "client = Client(\"127.0.0.1:8786\")\n",
    "client.run(trim_memory)\n",
    "client.restart()\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb6ac532-d64f-4659-9cc8-94481f48c366",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3b6eb9-e5d7-423a-a0bc-7b86e6db1ab4",
   "metadata": {},
   "source": [
    "## <font color='blue'> WRITE CODE BELOW </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea79a987-79ed-4ea9-a452-c4aa20b430b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import as_completed\n",
    "\n",
    "def get_supercat(cat_str):\n",
    "    try:\n",
    "        cat_list = ast.literal_eval(cat_str)\n",
    "        if isinstance(cat_list, list) and cat_list and isinstance(cat_list[0], list) and cat_list[0] and cat_list[0][0]:\n",
    "            return cat_list[0][0]\n",
    "    except:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def check_dangling(partition, products_subset):\n",
    "    merged = partition[['asin']].merge(products_subset, on='asin', how='left', indicator=True)\n",
    "    return int((merged['_merge'] == 'left_only').any().compute())\n",
    "\n",
    "def safe_extract(related_str):\n",
    "    try:\n",
    "        d = ast.literal_eval(related_str)\n",
    "        ids = []\n",
    "        for k in ['also_bought', 'also_viewed', 'bought_together', 'buy_after_viewing']:\n",
    "            val = d.get(k)\n",
    "            if isinstance(val, list):\n",
    "                ids.extend(val)\n",
    "        return ids\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def check_related_partition(partition, valid_asins_pd):\n",
    "    related_series = partition['related'].dropna().apply(safe_extract)\n",
    "    exploded = related_series.explode().dropna()\n",
    "    exploded_df = exploded.to_frame(name='asin')\n",
    "    merged = exploded_df.merge(valid_asins_pd, on='asin', how='left', indicator=True)\n",
    "    return int((merged['_merge'] == 'left_only').any().compute())\n",
    "\n",
    "reviews = dd.read_csv('user_reviews.csv', dtype={'asin': 'object'})\n",
    "products = dd.read_csv('products.csv', dtype={'asin': 'object'})\n",
    "\n",
    "reviews = reviews.persist()\n",
    "products = products.persist()\n",
    "\n",
    "### Question 1\n",
    "q1 = reviews.isna().sum() / len(reviews) * 100\n",
    "\n",
    "### Question 2\n",
    "q2 = products.isna().sum() / len(products) * 100\n",
    "merged = dd.merge(\n",
    "    reviews[['asin', 'overall']],\n",
    "    products[['asin', 'price']],\n",
    "    on='asin',\n",
    "    how='inner'\n",
    ").dropna(subset=['overall', 'price'])\n",
    "\n",
    "### Question 3\n",
    "q3 = merged[['overall', 'price']].corr('pearson',numeric_only=True)\n",
    "price = products['price'].dropna()\n",
    "\n",
    "### Question 4\n",
    "q4 = {\n",
    "    'mean':    price.mean(),\n",
    "    'std':     price.std(),\n",
    "    'min':     price.min(),\n",
    "    'max':     price.max(),\n",
    "    'median':  price.quantile(0.5)\n",
    "}\n",
    "\n",
    "### Question 5\n",
    "q5 = products['categories'].dropna().map(get_supercat, meta=('supercat', 'object')).value_counts()\n",
    "\n",
    "ans1_5 = dd.compute(q1,q2,q3,q4,q5)\n",
    "ans1_5\n",
    "\n",
    "### Question 6\n",
    "product_asins_df_future = client.scatter(products[['asin']], broadcast=True)\n",
    "\n",
    "delayed_parts = reviews.to_delayed()\n",
    "futures = [client.submit(check_dangling, part, product_asins_df_future) for part in delayed_parts]\n",
    "\n",
    "q6 = 0\n",
    "for fut in as_completed(futures):\n",
    "    if fut.result():\n",
    "        q6 = 1\n",
    "        break\n",
    "client.cancel(futures)\n",
    "\n",
    "### Question 7\n",
    "delayed_parts = products[['related']].dropna().to_delayed()\n",
    "futures = [client.submit(check_related_partition, part, product_asins_df_future) for part in delayed_parts]\n",
    "\n",
    "q7 = 0\n",
    "for fut in as_completed(futures):\n",
    "    if fut.result():\n",
    "        q7 = 1\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf416f4d-1782-4fa5-9b2b-b44aafd55934",
   "metadata": {},
   "outputs": [],
   "source": [
    "### read in the 'user_reviews.csv' and 'products.csv' files, perform your calculations and place the answers in variables ans1 - ans7.\n",
    "\n",
    "\n",
    "# substitute 'None' with the outputs from your calculations. \n",
    "# The expected output types can be seen in the assertion statements below\n",
    "ans1 = ans1_5[0].round(2).to_dict()\n",
    "ans2 = ans1_5[1].round(2).to_dict()\n",
    "ans3 = float(ans1_5[2].iloc[0,1].round(2))\n",
    "ans4 = {k: float(x.round(2)) for k, x in ans1_5[3].items()}\n",
    "ans5 = ans1_5[4].sort_values(ascending=False).to_dict()\n",
    "ans6 = q6\n",
    "ans7 = q7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d92954-28b3-4ad0-b7de-d8b8f4816c80",
   "metadata": {},
   "source": [
    "## <font color='red'> DO NOT MODIFY </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c438177d-8c4d-4871-bbc6-bea2f0a004b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0adca53b-b276-4297-8434-6c0e94810d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution time = 241.45133423805237s\n"
     ]
    }
   ],
   "source": [
    "print(f\"execution time = {end-start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "935be195-dcc9-4e97-911a-bae25e2a70f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "assert type(ans1) == dict, f\"answer to question 1 must be a dictionary like {{'reviewerID':0.2, ..}}, got type = {type(ans1)}\"\n",
    "assert type(ans2) == dict, f\"answer to question 2 must be a dictionary like {{'asin':0.2, ..}}, got type = {type(ans2)}\"\n",
    "assert type(ans3) == float, f\"answer to question 3 must be a float like 0.8, got type = {type(ans3)}\"\n",
    "# assert type(ans4) == dict, f\"answer to question 4 must be a dictionary like {{'mean':0.4,'max':0.6,'median':0.6...}}, got type = {type(ans4)}\"\n",
    "assert type(ans5) == dict, f\"answer to question 5 must be a dictionary, got type = {type(ans5)}\"         \n",
    "assert ans6 == 0 or ans6==1, f\"answer to question 6 must be 0 or 1, got value = {ans6}\" \n",
    "assert ans7 == 0 or ans7==1, f\"answer to question 7 must be 0 or 1, got value = {ans7}\" \n",
    "\n",
    "ans_dict = {\n",
    "    \"q1\": ans1,\n",
    "    \"q2\": ans2,\n",
    "    \"q3\": ans3,\n",
    "    \"q4\": ans4,\n",
    "    \"q5\": ans5,\n",
    "    \"q6\": ans6,\n",
    "    \"q7\": ans7,\n",
    "    \"runtime\": end-start\n",
    "}\n",
    "with open('my_results_PA1.json', 'w') as outfile: json.dump(ans_dict, outfile)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8205c2f-474b-4b48-9e54-0f4a6d4e1c99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
